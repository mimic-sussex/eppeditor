[{"2":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":1,"y":0,"w":1,"h":2},"3":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":2,"y":0,"w":1,"h":2},"6":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":4,"y":0,"w":2,"h":2,"id":"_mtcxvez7a"},"8":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":5,"y":0,"w":3,"h":2,"id":"_mtcxvez7a"},"12":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":7,"y":0,"w":5,"h":2,"id":"_mtcxvez7a"},"id":"_7a8vbfchn","data":{"id":"_7a8vbfchn","type":"analyser","name":"analyser_7a8vbfchn","background":"#000000","lineNumbers":true,"hasFocus":true,"theme":"icecoder","mode":""},"hasFocus":false},{"2":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":0,"y":2,"w":1,"h":2},"3":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":2,"y":2,"w":1,"h":5,"id":"_ae99mi2yd"},"6":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":4,"y":2,"w":2,"h":5,"id":"_ae99mi2yd"},"8":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":5,"y":2,"w":3,"h":5,"id":"_ae99mi2yd"},"12":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":7,"y":2,"w":5,"h":5,"id":"_ae99mi2yd"},"id":"_5493sodhp","data":{"id":"_5493sodhp","type":"grammarEditor","name":"grammarEditor_5493sodhp","background":"#151515","lineNumbers":true,"hasFocus":true,"theme":"monokai","grammarSource":"/languages/default/grammar.ne","content":"# Lexer [or tokenizer] definition with language lexemes [or tokens]\n@{%\n\nconst lexer = moo.compile({\n  separator:      /,/,\n  paramEnd:       /}/,\n  paramBegin:     /{/,\n  listEnd:        /\\]/,\n  listBegin:      /\\[/,\n  dacoutCh:       /\\>[0-9]+/,\n  dacout:         /\\>/,\n  variable:       /:[a-zA-Z0-9]+:/,\n  sample:         { match: /\\\\[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  slice:          { match: /\\|[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  stretch:        { match: /\\@[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  clockTrig:      /0t-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n\tnumber:         /-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n  semicolon:      /;/,\n  funcName:       /[a-zA-Z][a-zA-Z0-9]*/,\n\tstring:\t\t\t\t\t{ match: /'[a-zA-Z0-9]+'/, value: x => x.slice(1,x.length-1)},\n  comment:        /\\/\\/[^\\n]*/,\n  ws:             { match: /\\s+/, lineBreaks: true},\n});\n\n%}\n\n# Pass your lexer object using the @lexer option\n@lexer lexer\n\n# Grammar definition in the Extended Backus Naur Form (EBNF)\nmain -> _ Statement _\n{% d => ( { '@lang' : d[1] } )  %}\n\nStatement ->\n  %comment _ Statement\n  {% d => d[2] %}\n\t|\n  Expression _ %semicolon _ Statement\n  {% d => [ { '@spawn': d[0] } ].concat(d[4]) %}\n  |\n  Expression _ %semicolon (_ %comment):*\n  {% d => [ { '@spawn': d[0] } ] %}\n\n\nExpression ->\n  ParameterList _ %funcName\n  {% d => sema.synth( d[2].value, d[0]['@params'] ) %}\n  |\n  ParameterList _ %sample\n  {% d => sema.synth( 'sampler', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %slice\n  {% d => sema.synth( 'slice', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %stretch\n  {% d => sema.synth( 'stretch', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  %variable _ Expression\n  {% d => sema.setvar( d[0].value, d[2] ) %}\n  |\n  %dacout _ Expression\n  {% d => sema.synth( 'dac', [d[2]] ) %}\n  |\n  %dacoutCh _ Expression\n  {% d => sema.synth( 'dac', [d[2], sema.num(d[0].value.substr(1))] ) %}\n\nParameterList ->\n  %paramBegin Params %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': d[1], 'paramEnd': d[2] } ) %}\n\t|\n\t%paramBegin _ %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': [], 'paramEnd': d[2] } ) %}\n\n\nParams ->\n  ParamElement\n  {% d => ( [ d[0] ] ) %}\n  |\n  ParamElement _ %separator _ Params\n  {% d => [ d[0] ].concat(d[4]) %}\n\nParamElement ->\n  %number\n  {% d => ( { '@num': d[0] } ) %}\n\t|\n\t%string\n  {% d => ( { '@string': d[0].value } ) %}\n  |\n  Expression\n  {% id %}\n  |\n  %variable\n  {% d => sema.getvar( d[0].value ) %}\n  |\n  %listBegin Params  %listEnd\n  {% d => ( { '@list': d[1] } )%}\n\n\n# Whitespace\n\n_  -> wschar:*\n{% function(d) {return null;} %}\n\n__ -> wschar:+\n{% function(d) {return null;} %}\n\nwschar -> %ws\n{% id %}\n"},"hasFocus":false},{"2":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":0,"y":0,"w":1,"h":2},"3":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":0,"y":0,"w":2,"h":7,"id":"_cctdhevvf"},"6":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":0,"y":0,"w":4,"h":7,"id":"_cctdhevvf"},"8":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":0,"y":0,"w":5,"h":5,"id":"_2cmtupcut"},"12":{"fixed":false,"resizable":true,"draggable":true,"customDragger":false,"customResizer":false,"min":{"w":1,"h":1},"max":{},"x":0,"y":0,"w":7,"h":7,"id":"_cctdhevvf"},"id":"_p3i67hrdk","data":{"id":"_p3i67hrdk","type":"liveCodeEditor","name":"liveCodeEditor_p3i67hrdk","background":"#151515","lineNumbers":true,"hasFocus":true,"theme":"icecoder","grammarSource":"/languages/default/grammar.ne","liveCodeSource":"","content":"{{{50}sin, 110, 120}ulin,4}clk;\n:channel1: {{8}clt}\\spade;\n:channel2: {{1}clt}\\909b;\n:channel3: {{2,0.50}clt}\\909;\n:channel4: {{0.25}clt}\\909open;\n> {:channel1:,:channel2:,:channel3:,:channel4:}mix;","grammar":"# Lexer [or tokenizer] definition with language lexemes [or tokens]\n@{%\n\nconst lexer = moo.compile({\n  separator:      /,/,\n  paramEnd:       /}/,\n  paramBegin:     /{/,\n  listEnd:        /\\]/,\n  listBegin:      /\\[/,\n  dacoutCh:       /\\>[0-9]+/,\n  dacout:         /\\>/,\n  variable:       /:[a-zA-Z0-9]+:/,\n  sample:         { match: /\\\\[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  slice:          { match: /\\|[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  stretch:        { match: /\\@[a-zA-Z0-9]+/, lineBreaks: true, value: x => x.slice(1, x.length)},\n  clockTrig:      /0t-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n\tnumber:         /-?(?:[0-9]|[1-9][0-9]+)(?:\\.[0-9]+)?\\b/,\n  semicolon:      /;/,\n  funcName:       /[a-zA-Z][a-zA-Z0-9]*/,\n\tstring:\t\t\t\t\t{ match: /'[a-zA-Z0-9]+'/, value: x => x.slice(1,x.length-1)},\n  comment:        /\\/\\/[^\\n]*/,\n  ws:             { match: /\\s+/, lineBreaks: true},\n});\n\n%}\n\n# Pass your lexer object using the @lexer option\n@lexer lexer\n\n# Grammar definition in the Extended Backus Naur Form (EBNF)\nmain -> _ Statement _\n{% d => ( { '@lang' : d[1] } )  %}\n\nStatement ->\n  %comment _ Statement\n  {% d => d[2] %}\n\t|\n  Expression _ %semicolon _ Statement\n  {% d => [ { '@spawn': d[0] } ].concat(d[4]) %}\n  |\n  Expression _ %semicolon (_ %comment):*\n  {% d => [ { '@spawn': d[0] } ] %}\n\n\nExpression ->\n  ParameterList _ %funcName\n  {% d => sema.synth( d[2].value, d[0]['@params'] ) %}\n  |\n  ParameterList _ %sample\n  {% d => sema.synth( 'sampler', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %slice\n  {% d => sema.synth( 'slice', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  ParameterList _ %stretch\n  {% d => sema.synth( 'stretch', d[0]['@params'].concat( [ sema.str( d[2].value ) ] ) ) %}\n  |\n  %variable _ Expression\n  {% d => sema.setvar( d[0].value, d[2] ) %}\n  |\n  %dacout _ Expression\n  {% d => sema.synth( 'dac', [d[2]] ) %}\n  |\n  %dacoutCh _ Expression\n  {% d => sema.synth( 'dac', [d[2], sema.num(d[0].value.substr(1))] ) %}\n\nParameterList ->\n  %paramBegin Params %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': d[1], 'paramEnd': d[2] } ) %}\n\t|\n\t%paramBegin _ %paramEnd\n  {% d => ( { 'paramBegin': d[0], '@params': [], 'paramEnd': d[2] } ) %}\n\n\nParams ->\n  ParamElement\n  {% d => ( [ d[0] ] ) %}\n  |\n  ParamElement _ %separator _ Params\n  {% d => [ d[0] ].concat(d[4]) %}\n\nParamElement ->\n  %number\n  {% d => ( { '@num': d[0] } ) %}\n\t|\n\t%string\n  {% d => ( { '@string': d[0].value } ) %}\n  |\n  Expression\n  {% id %}\n  |\n  %variable\n  {% d => sema.getvar( d[0].value ) %}\n  |\n  %listBegin Params  %listEnd\n  {% d => ( { '@list': d[1] } )%}\n\n\n# Whitespace\n\n_  -> wschar:*\n{% function(d) {return null;} %}\n\n__ -> wschar:+\n{% function(d) {return null;} %}\n\nwschar -> %ws\n{% id %}\n"},"hasFocus":true}]